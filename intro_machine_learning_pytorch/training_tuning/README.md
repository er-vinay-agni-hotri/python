## Training and Tuning
### Glossary
| Key Term|	Definition |
|-----|---------|
|Bias	|The bias is known as the difference between the prediction of the values by the ML model and the correct value. High bias reslts in a large error in training as well as testing data.
|Grid search	|A table with results of all probabilities and the best result is used.
|K-fold cross validation|	A parameter called 'k' that represents the number of groups that a given data sample is to be split into. Then the results for all subsets are averaged.
|Learning curves|	A tool used in machine learning that learns from a training dataset incrementally, and the plot shows changes in learning performance over time.
|Overfitting	|When a model uses a lines which is too complex, as if memorizing the training set, and won't generalize well. Over-complication of the problem
|Underfitting	|When a model uses a line, which is too simplistic. It doesn't do very well on the training set. Oversimplification of the problem
|Variance	|iA type of error due to a model's sensitivity to small fluctuations in the training set. High variance would cause an algorithm to model the noise in the training set. This is known as overfitting
